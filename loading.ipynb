{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 columns processed...\n",
      "1000 columns processed...\n",
      "1500 columns processed...\n",
      "500 columns processed...\n",
      "1000 columns processed...\n",
      "1500 columns processed...\n"
     ]
    }
   ],
   "source": [
    "# Run dependencies\n",
    "%run transformation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************LOADING*******************************************\n",
    "import psycopg2\n",
    "%run config.ipynb\n",
    "engine=create_engine(f\"postgresql://{connection_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Independence Day</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanksgiving Day</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christmas Eve</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christmas Day</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Holiday  Year       Date\n",
       "0    New Year's Day  2010 2010-01-01\n",
       "1  Independence Day  2010 2010-07-04\n",
       "2  Thanksgiving Day  2010 2010-11-25\n",
       "3     Christmas Eve  2010 2010-12-24\n",
       "4     Christmas Day  2010 2010-12-25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df.to_sql(name=\"holiday\", con=engine, if_exists='append', index=True)\n",
    "\n",
    "walmart.to_sql(name=\"walmart\", con=engine, if_exists='append', index=True)\n",
    "\n",
    "stock.to_sql(name=\"stock\", con=engine, if_exists='append', index=True)\n",
    "\n",
    "state_id.to_sql(name=\"state_id\", con=engine, if_exists='append', index=True)\n",
    "\n",
    "store.to_sql(name=\"store\", con=engine, if_exists='append', index=True)\n",
    "\n",
    "marketShare_final.to_sql(name=\"marketshare\", con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add primary keys\n",
    "engine.execute('ALTER TABLE \"CAfoods\" ADD PRIMARY KEY (id)')\n",
    "engine.execute('ALTER TABLE state_category ADD PRIMARY KEY (state, cat_id)')\n",
    "engine.execute('ALTER TABLE walmart_stores ADD PRIMARY KEY (state)')\n",
    "engine.execute('ALTER TABLE walmart_state_cat ADD PRIMARY KEY (state, cat_id)')\n",
    "engine.execute('ALTER TABLE state_store_sale ADD PRIMARY KEY (state)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.to_csv('Resources/clean/d3_sales.csv'),index=False)\n",
    "df_price_changes.to_csv('Resources/clean/d3_price_changes.csv'),index=False)\n",
    "\n",
    "df_sales_categories.to_csv('Resources/clean/d3_categories.csv'),index=False)\n",
    "df_sales_departments.to_csv('Resources/clean/d3_departments.csv'),index=False)\n",
    "df_sales_stores.to_csv('Resources/clean/d3_stores.csv'),index=False)\n",
    "df_sales_states.to_csv('Resources/clean/d3_states.csv'),index=False)\n",
    "df_sales_items.to_csv('Resources/clean/d3_items.csv'),index=False)\n",
    "\n",
    "df_ecomm.to_csv('Resources/clean/d7_ecomm.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code loops through the folder of cleaned .csv files and loads them to PostgreSQL\n",
    "# this is over 20 faster than using sqlalchemy and df.to_sql for long tables\n",
    "\n",
    "# files are read to memory using StringIO in the io package\n",
    "# 'copy [table] from stdin' in PostgreSQL, which directly from memory on the local computer\n",
    "\n",
    "folder_name = os.path.join('Resources/clean')\n",
    "\n",
    "conn_host = 'otto.db.elephantsql.com'\n",
    "conn_dbname = 'ofiglsqd'\n",
    "conn_user = 'ofiglsqd'\n",
    "\n",
    "conn_pass = getpass.getpass(prompt='password: ')\n",
    "\n",
    "# loop through .csv files in the output folder\n",
    "for file in os.listdir(folder_name):\n",
    "\n",
    "    print('\\n\\n' + str(datetime.utcnow()) + ' ' + str(file) + ' to be loaded')\n",
    "\n",
    "    print(str(datetime.utcnow()) + ' reading file to dataframe...')\n",
    "    \n",
    "    # read .csv file into dataframe\n",
    "    df = pd.read_csv(os.path.join(folder_name, file), na_values=['nan','NA','NaN'])\n",
    "    \n",
    "    print(str(datetime.utcnow()) + ' completed')\n",
    "    \n",
    "    print(df.info(memory_usage='deep'))\n",
    "    \n",
    "    # \n",
    "    with psycopg2.connect(host=conn_host, dbname=conn_dbname, user=conn_user, password=conn_pass) as conn:\n",
    "        conn.autocommit = True\n",
    "\n",
    "        table_name = file.split('.csv')[0].lower().replace('-','_')\n",
    "\n",
    "        output = io.StringIO()\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' reading file to memory using StringIO...')\n",
    "\n",
    "        df.to_csv(output, sep='|', header=False, index=False)\n",
    "        output.seek(0)\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' completed')\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' generating the create table statement...')\n",
    "        \n",
    "        qry = pd.io.sql.get_schema(df, table_name, con=conn)\n",
    "\n",
    "        qry = qry.replace('CREATE TABLE', 'CREATE TABLE IF NOT EXISTS')\n",
    "\n",
    "        for key in df.columns:\n",
    "            if pd.api.types.infer_dtype(df[key], skipna=True) == 'boolean':\n",
    "                start = qry.find(key)\n",
    "                end = start + qry[start:].find(',')\n",
    "                print(start, end)\n",
    "                qry = qry[:start] + key + '\" BOOLEAN' + qry[end:]\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "                print(qry)\n",
    "                \n",
    "                print(str(datetime.utcnow()) + ' executing the create table statement...')\n",
    "                cur.execute(qry)\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "                \n",
    "                print(str(datetime.utcnow()) + ' loading table to database...')\n",
    "                cur.copy_expert(\"\"\"COPY %s FROM STDIN WITH (FORMAT csv, DELIMITER '|', QUOTE '\"')\"\"\" % table_name, output)\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error:\\n' + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add financial data tables\n",
    "revenue_df.to_sql(name='revenue', con=engine, if_exists='append', index=True)\n",
    "netincome_df.to_sql(name='net_income', con=engine, if_exists='append', index=True)\n",
    "earnings_df.to_sql(name='earnings', con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tables in database\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
