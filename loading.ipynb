{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 columns processed...\n",
      "1000 columns processed...\n",
      "1500 columns processed...\n",
      "500 columns processed...\n",
      "1000 columns processed...\n",
      "1500 columns processed...\n"
     ]
    }
   ],
   "source": [
    "# Run dependencies\n",
    "%run transformation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************LOADING*******************************************\n",
    "import psycopg2\n",
    "engine=create_engine(f\"postgresql://{connection_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into SQL\n",
    "holiday_df.to_sql(name=\"holiday\", con=engine, if_exists='append', index=False)\n",
    "\n",
    "walmart.to_sql(name=\"walmart\", con=engine, if_exists='append', index=False)\n",
    "\n",
    "stock.to_sql(name=\"stock\", con=engine, if_exists='append', index=False)\n",
    "\n",
    "state_id.to_sql(name=\"state_id\", con=engine, if_exists='append', index=False)\n",
    "\n",
    "store.to_sql(name=\"store\", con=engine, if_exists='append', index=False)\n",
    "\n",
    "marketShare_final.to_sql(name=\"marketshare\", con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add primary keys\n",
    "engine.execute('ALTER TABLE \"store\" ADD PRIMARY KEY (id)')\n",
    "engine.execute('ALTER TABLE \"walmart\" ADD PRIMARY KEY (\"ID\")')\n",
    "engine.execute('ALTER TABLE \"holiday\" ADD PRIMARY KEY (Date)')\n",
    "engine.execute('ALTER TABLE \"marketshare\" ADD PRIMARY KEY (CITY)')\n",
    "engine.execute('ALTER TABLE \"state_id\" ADD PRIMARY KEY (state_id)')\n",
    "engine.execute('ALTER TABLE \"stock\" ADD PRIMARY KEY (Date)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv files\n",
    "df_sales.to_csv(('Resources/clean/d3_sales.csv'),index=False)\n",
    "df_price_changes.to_csv(('Resources/clean/d3_price_changes.csv'),index=False)\n",
    "df_sales_categories.to_csv(('Resources/clean/d3_categories.csv'),index=False)\n",
    "df_sales_stores.to_csv(('Resources/clean/d3_stores.csv'),index=False)\n",
    "df_sales_items.to_csv(('Resources/clean/d3_items.csv'),index=False)\n",
    "df_ecomm.to_csv(('Resources/clean/d7_ecomm.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import getpass\n",
    "# this code loops through the folder of cleaned .csv files and loads them to PostgreSQL\n",
    "# this is over 20 faster than using sqlalchemy and df.to_sql for long tables\n",
    "\n",
    "# files are read to memory using StringIO in the io package\n",
    "# 'copy [table] from stdin' in PostgreSQL, which directly from memory on the local computer\n",
    "\n",
    "folder_name = os.path.join('Resources/clean')\n",
    "\n",
    "conn_host = 'otto.db.elephantsql.com'\n",
    "conn_dbname = 'ofiglsqd'\n",
    "conn_user = 'ofiglsqd'\n",
    "\n",
    "#conn_pass = getpass.getpass(prompt='password: ')\n",
    "\n",
    "# loop through .csv files in the output folder\n",
    "for file in os.listdir(folder_name):\n",
    "\n",
    "    print('\\n\\n' + str(datetime.utcnow()) + ' ' + str(file) + ' to be loaded')\n",
    "\n",
    "    print(str(datetime.utcnow()) + ' reading file to dataframe...')\n",
    "    \n",
    "    # read .csv file into dataframe\n",
    "    df = pd.read_csv(os.path.join(folder_name, file), na_values=['nan','NA','NaN'])\n",
    "    \n",
    "    print(str(datetime.utcnow()) + ' completed')\n",
    "    \n",
    "    print(df.info(memory_usage='deep'))\n",
    "    \n",
    "    # \n",
    "    with psycopg2.connect(host=conn_host, dbname=conn_dbname, user=conn_user, password=conn_pass) as conn:\n",
    "        conn.autocommit = True\n",
    "\n",
    "        table_name = file.split('.csv')[0].lower().replace('-','_')\n",
    "\n",
    "        output = io.StringIO()\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' reading file to memory using StringIO...')\n",
    "\n",
    "        df.to_csv(output, sep='|', header=False, index=False)\n",
    "        output.seek(0)\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' completed')\n",
    "\n",
    "        print(str(datetime.utcnow()) + ' generating the create table statement...')\n",
    "        \n",
    "        qry = pd.io.sql.get_schema(df, table_name, con=conn)\n",
    "\n",
    "        qry = qry.replace('CREATE TABLE', 'CREATE TABLE IF NOT EXISTS')\n",
    "\n",
    "        for key in df.columns:\n",
    "            if pd.api.types.infer_dtype(df[key], skipna=True) == 'boolean':\n",
    "                start = qry.find(key)\n",
    "                end = start + qry[start:].find(',')\n",
    "                print(start, end)\n",
    "                qry = qry[:start] + key + '\" BOOLEAN' + qry[end:]\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "                print(qry)\n",
    "                \n",
    "                print(str(datetime.utcnow()) + ' executing the create table statement...')\n",
    "                cur.execute(qry)\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "                \n",
    "                print(str(datetime.utcnow()) + ' loading table to database...')\n",
    "                cur.copy_expert(\"\"\"COPY %s FROM STDIN WITH (FORMAT csv, DELIMITER '|', QUOTE '\"')\"\"\" % table_name, output)\n",
    "                print(str(datetime.utcnow()) + ' completed')\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error:\\n' + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add primary key\n",
    "engine.execute('ALTER TABLE \"d3_sales\" ADD PRIMARY KEY (\"index\")')\n",
    "engine.execute('ALTER TABLE \"d3_stores\" ADD PRIMARY KEY (store_id)')\n",
    "engine.execute('ALTER TABLE \"d3_price_changes\" ADD PRIMARY KEY (index)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add primary key\n",
    "engine.execute('ALTER TABLE \"d3_categories\" ADD PRIMARY KEY (cat_id)')\n",
    "engine.execute('ALTER TABLE \"d3_items\" ADD PRIMARY KEY (item_id)')\n",
    "engine.execute('ALTER TABLE \"d7_ecomm\" ADD PRIMARY KEY (ecomm_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add financial data tables\n",
    "revenue_df.to_sql(name='revenue', con=engine, if_exists='append', index=True)\n",
    "netincome_df.to_sql(name='net_income', con=engine, if_exists='append', index=True)\n",
    "earnings_df.to_sql(name='earnings', con=engine, if_exists='append', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add primary key\n",
    "engine.execute('ALTER TABLE \"earnings\" ADD PRIMARY KEY (date)')\n",
    "engine.execute('ALTER TABLE \"net_income\" ADD PRIMARY KEY (date)')\n",
    "engine.execute('ALTER TABLE \"revenue\" ADD PRIMARY KEY (date)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tables in database\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
